{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import numpy as np # linear algebra\n",
    "import math\n",
    "# %pylab inline\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_features = pd.read_csv('dataset_video.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.get_values of            0       1       2   3   4         5         6         7         8   \\\n",
       "0    1005.jpg  171904  266229   1   8  3.609327  5.786311  0.061507  0.000018   \n",
       "1    1020.jpg  174374  269980   1   8  3.617222  5.808520  0.061179  0.000018   \n",
       "2    1035.jpg  166952  259987   1   8  3.666094  5.768350  0.062114  0.000019   \n",
       "3    1050.jpg  178297  274452   1   8  3.717024  5.763923  0.059718  0.000017   \n",
       "4    1065.jpg  181291  280864   1   8  3.657921  5.774430  0.062070  0.000017   \n",
       "5    1080.jpg  176952  276148   1   8  3.683201  5.740675  0.063021  0.000018   \n",
       "6    1095.jpg  174420  274301   1   8  3.764835  5.746088  0.065768  0.000018   \n",
       "7    1110.jpg  167198  262659   1   8  3.757893  5.755775  0.064974  0.000019   \n",
       "8    1125.jpg  167179  263125   1   8  3.808829  5.664751  0.066448  0.000019   \n",
       "9    1140.jpg  166668  262601   1   8  3.765475  5.714957  0.066233  0.000019   \n",
       "10   1155.jpg  166932  262708   1   8  3.761280  5.721943  0.065901  0.000019   \n",
       "11   1170.jpg  167089  263258   1   8  3.722749  5.763434  0.064886  0.000019   \n",
       "12   1185.jpg  167681  263775   1   8  3.661897  5.742976  0.064406  0.000019   \n",
       "13    300.jpg  152570  249555   1   8  3.661074  5.796653  0.071008  0.000021   \n",
       "14    315.jpg  156281  254451   1   8  3.668824  5.800505  0.068240  0.000021   \n",
       "15    330.jpg  156607  254657   1   8  3.640990  5.811692  0.067914  0.000021   \n",
       "16    345.jpg  155594  253261   1   8  3.649463  5.781444  0.067466  0.000021   \n",
       "17    360.jpg  156420  254940   1   8  3.649870  5.793144  0.068374  0.000021   \n",
       "18    375.jpg  155718  253429   1   8  3.656584  5.803740  0.067740  0.000021   \n",
       "19    390.jpg  156925  255263   1   8  3.636619  5.810492  0.067622  0.000021   \n",
       "20    405.jpg  155162  252947   1   8  3.638608  5.794406  0.067460  0.000021   \n",
       "21    420.jpg  156325  254196   1   8  3.635622  5.786652  0.067117  0.000021   \n",
       "22    435.jpg  157534  254973   1   8  3.615162  5.791466  0.066843  0.000021   \n",
       "23    450.jpg  163322  260904   1   8  3.592988  5.795574  0.064274  0.000020   \n",
       "24    465.jpg  157182  251232   1   8  3.625134  5.800457  0.065577  0.000020   \n",
       "25    480.jpg  160400  255803   1   8  3.646004  5.773742  0.065673  0.000020   \n",
       "26    495.jpg  155524  250497   1   8  3.691797  5.791023  0.067073  0.000021   \n",
       "27    510.jpg  156867  249040   1   8  3.618942  5.799595  0.065896  0.000020   \n",
       "28    525.jpg  162646  254809   1   8  3.594684  5.750208  0.064827  0.000019   \n",
       "29    540.jpg  167021  260478   1   8  3.601304  5.735860  0.063321  0.000019   \n",
       "..        ...     ...     ...  ..  ..       ...       ...       ...       ...   \n",
       "394  6210.jpg  280695  471377   1   8  3.816175  5.351665  0.073540  0.000012   \n",
       "395  6225.jpg  273632  460024   1   8  3.824547  5.327244  0.072366  0.000012   \n",
       "396  6240.jpg  277916  467398   1   8  3.817898  5.337601  0.072847  0.000012   \n",
       "397  6255.jpg  274257  461262   1   8  3.819898  5.353036  0.072627  0.000012   \n",
       "398  6270.jpg  279513  470073   1   8  3.817464  5.323835  0.073779  0.000012   \n",
       "399  6285.jpg  277807  467211   1   8  3.816905  5.370519  0.073201  0.000012   \n",
       "400  6300.jpg  278990  468724   1   8  3.811118  5.375510  0.073748  0.000012   \n",
       "401  6315.jpg  278295  467561   1   8  3.804937  5.329398  0.074097  0.000012   \n",
       "402  6330.jpg  279923  470960   1   8  3.805450  5.333455  0.074705  0.000012   \n",
       "403  6345.jpg  278490  471519   1   8  3.814503  5.365174  0.074197  0.000012   \n",
       "404  6360.jpg  278389  469554   1   8  3.787673  5.333237  0.073479  0.000012   \n",
       "405  6375.jpg  273703  462058   1   8  3.812826  5.344091  0.072296  0.000012   \n",
       "406  6390.jpg  277720  469620   1   8  3.793241  5.352634  0.072877  0.000012   \n",
       "407  6405.jpg  274232  462775   1   8  3.779937  5.354389  0.072101  0.000012   \n",
       "408  6420.jpg  279554  470888   1   8  3.785772  5.402519  0.073262  0.000012   \n",
       "409  6435.jpg  272976  461495   1   8  3.760980  5.383374  0.073414  0.000012   \n",
       "410  6450.jpg  276255  466332   1   8  3.768038  5.363678  0.073793  0.000012   \n",
       "411  6465.jpg  272605  459249   1   8  3.756570  5.363644  0.072978  0.000012   \n",
       "412  6480.jpg  276881  466253   1   8  3.758342  5.370968  0.073546  0.000012   \n",
       "413  6495.jpg  272881  460037   1   8  3.767447  5.308398  0.073724  0.000012   \n",
       "414  6510.jpg  276622  467707   1   8  3.774810  5.354868  0.074691  0.000012   \n",
       "415  6525.jpg  273237  462117   1   8  3.782927  5.344177  0.074479  0.000012   \n",
       "416  6540.jpg  277016  467500   1   8  3.755719  5.344537  0.074345  0.000012   \n",
       "417  6555.jpg  274796  463540   1   8  3.794749  5.361356  0.073975  0.000012   \n",
       "418  6570.jpg  277857  467674   1   8  3.800184  5.341525  0.073733  0.000012   \n",
       "419  6585.jpg  274670  462027   1   8  3.806848  5.353298  0.073503  0.000012   \n",
       "420  6600.jpg  279124  469112   1   8  3.806069  5.348458  0.075304  0.000012   \n",
       "421  6615.jpg  272836  459340   1   8  3.799813  5.358672  0.074923  0.000012   \n",
       "422  6630.jpg  275439  463090   1   8  3.791029  5.368632  0.075127  0.000012   \n",
       "423  6645.jpg  274143  460951   1   8  3.793803  5.373070  0.075174  0.000012   \n",
       "\n",
       "           9         10  11  12        13  14  \n",
       "0    0.000470  0.231792   0   1  0.231792   0  \n",
       "1    0.000480  0.223140   0   1  0.223140   0  \n",
       "2    0.000490  0.215881   0   1  0.215881   0  \n",
       "3    0.000473  0.234164   0   1  0.234164   0  \n",
       "4    0.000467  0.239211   0   1  0.239211   0  \n",
       "5    0.000450  0.234612   0   1  0.234612   0  \n",
       "6    0.000437  0.227402   0   1  0.227402   0  \n",
       "7    0.000483  0.217092   0   1  0.217092   0  \n",
       "8    0.000449  0.208020   0   1  0.208020   0  \n",
       "9    0.000452  0.210575   0   1  0.210575   0  \n",
       "10   0.000449  0.211857   0   1  0.211857   0  \n",
       "11   0.000462  0.218142   0   1  0.218142   0  \n",
       "12   0.000461  0.221229   0   1  0.221229   0  \n",
       "13   0.000483  0.208400   0   1  0.208400   0  \n",
       "14   0.000465  0.236315   0   1  0.236315   0  \n",
       "15   0.000472  0.239431   0   1  0.239431   0  \n",
       "16   0.000472  0.236183   0   1  0.236183   0  \n",
       "17   0.000472  0.235445   0   1  0.235445   0  \n",
       "18   0.000478  0.236497   0   1  0.236497   0  \n",
       "19   0.000477  0.238428   0   1  0.238428   0  \n",
       "20   0.000468  0.235036   0   1  0.235036   0  \n",
       "21   0.000474  0.237034   0   1  0.237034   0  \n",
       "22   0.000472  0.242520   0   1  0.242520   0  \n",
       "23   0.000477  0.247858   0   1  0.247858   0  \n",
       "24   0.000497  0.234863   0   1  0.234863   0  \n",
       "25   0.000477  0.224834   0   1  0.224834   0  \n",
       "26   0.000478  0.217243   0   1  0.217243   0  \n",
       "27   0.000486  0.234268   0   1  0.234268   0  \n",
       "28   0.000491  0.227498   0   1  0.227498   0  \n",
       "29   0.000482  0.225442   0   1  0.225442   0  \n",
       "..        ...       ...  ..  ..       ...  ..  \n",
       "394  0.000267  0.242551   0   1  0.242551   1  \n",
       "395  0.000298  0.235268   0   1  0.235268   1  \n",
       "396  0.000282  0.237695   0   1  0.237695   1  \n",
       "397  0.000286  0.239865   0   1  0.239865   1  \n",
       "398  0.000268  0.240768   0   1  0.240768   1  \n",
       "399  0.000279  0.236690   0   1  0.236690   1  \n",
       "400  0.000276  0.239600   0   1  0.239600   1  \n",
       "401  0.000271  0.234771   0   1  0.234771   1  \n",
       "402  0.000268  0.236012   0   1  0.236012   1  \n",
       "403  0.000270  0.235580   0   1  0.235580   1  \n",
       "404  0.000271  0.239373   0   1  0.239373   1  \n",
       "405  0.000284  0.236861   0   1  0.236861   1  \n",
       "406  0.000275  0.237601   0   1  0.237601   1  \n",
       "407  0.000270  0.235835   0   1  0.235835   1  \n",
       "408  0.000279  0.241079   0   1  0.241079   1  \n",
       "409  0.000279  0.241095   0   1  0.241095   1  \n",
       "410  0.000272  0.240840   0   1  0.240840   1  \n",
       "411  0.000279  0.239690   0   1  0.239690   1  \n",
       "412  0.000277  0.238842   0   1  0.238842   1  \n",
       "413  0.000274  0.237460   0   1  0.237460   1  \n",
       "414  0.000266  0.239418   0   1  0.239418   1  \n",
       "415  0.000271  0.235408   0   1  0.235408   1  \n",
       "416  0.000267  0.238109   0   1  0.238109   1  \n",
       "417  0.000281  0.236694   0   1  0.236694   1  \n",
       "418  0.000267  0.236712   0   1  0.236712   1  \n",
       "419  0.000276  0.237211   0   1  0.237211   1  \n",
       "420  0.000263  0.237393   0   1  0.237393   1  \n",
       "421  0.000280  0.237402   0   1  0.237402   1  \n",
       "422  0.000272  0.240896   0   1  0.240896   1  \n",
       "423  0.000270  0.238516   0   1  0.238516   1  \n",
       "\n",
       "[424 rows x 15 columns]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_features.get_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>215550.818396</td>\n",
       "      <td>352342.801887</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.712930</td>\n",
       "      <td>5.582442</td>\n",
       "      <td>0.067684</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.230533</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.230533</td>\n",
       "      <td>0.528302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47913.231106</td>\n",
       "      <td>89341.026630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.096709</td>\n",
       "      <td>0.206506</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.015637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015637</td>\n",
       "      <td>0.509160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>143681.000000</td>\n",
       "      <td>209739.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.694517</td>\n",
       "      <td>5.066331</td>\n",
       "      <td>0.038356</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000259</td>\n",
       "      <td>0.187878</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.187878</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>167887.750000</td>\n",
       "      <td>264268.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.645971</td>\n",
       "      <td>5.363422</td>\n",
       "      <td>0.063788</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.218083</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.218083</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>240172.000000</td>\n",
       "      <td>397536.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.723091</td>\n",
       "      <td>5.532734</td>\n",
       "      <td>0.068326</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.236406</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236406</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>260476.250000</td>\n",
       "      <td>438408.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.791016</td>\n",
       "      <td>5.791119</td>\n",
       "      <td>0.072263</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.242970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.242970</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>286463.000000</td>\n",
       "      <td>481198.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.847907</td>\n",
       "      <td>5.927963</td>\n",
       "      <td>0.076511</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.264631</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.264631</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  1              2      3      4           5           6   \\\n",
       "count     424.000000     424.000000  424.0  424.0  424.000000  424.000000   \n",
       "mean   215550.818396  352342.801887    1.0    8.0    3.712930    5.582442   \n",
       "std     47913.231106   89341.026630    0.0    0.0    0.096709    0.206506   \n",
       "min    143681.000000  209739.000000    1.0    8.0    2.694517    5.066331   \n",
       "25%    167887.750000  264268.000000    1.0    8.0    3.645971    5.363422   \n",
       "50%    240172.000000  397536.000000    1.0    8.0    3.723091    5.532734   \n",
       "75%    260476.250000  438408.500000    1.0    8.0    3.791016    5.791119   \n",
       "max    286463.000000  481198.000000    1.0    8.0    3.847907    5.927963   \n",
       "\n",
       "               7           8           9           10     11     12  \\\n",
       "count  424.000000  424.000000  424.000000  424.000000  424.0  424.0   \n",
       "mean     0.067684    0.000016    0.000387    0.230533    0.0    1.0   \n",
       "std      0.005091    0.000003    0.000107    0.015637    0.0    0.0   \n",
       "min      0.038356    0.000012    0.000259    0.187878    0.0    1.0   \n",
       "25%      0.063788    0.000013    0.000292    0.218083    0.0    1.0   \n",
       "50%      0.068326    0.000014    0.000338    0.236406    0.0    1.0   \n",
       "75%      0.072263    0.000019    0.000472    0.242970    0.0    1.0   \n",
       "max      0.076511    0.000021    0.001187    0.264631    0.0    1.0   \n",
       "\n",
       "               13          14  \n",
       "count  424.000000  424.000000  \n",
       "mean     0.230533    0.528302  \n",
       "std      0.015637    0.509160  \n",
       "min      0.187878    0.000000  \n",
       "25%      0.218083    0.000000  \n",
       "50%      0.236406    1.000000  \n",
       "75%      0.242970    1.000000  \n",
       "max      0.264631    2.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 424 entries, 0 to 423\n",
      "Data columns (total 15 columns):\n",
      "0     424 non-null object\n",
      "1     424 non-null int64\n",
      "2     424 non-null int64\n",
      "3     424 non-null int64\n",
      "4     424 non-null int64\n",
      "5     424 non-null float64\n",
      "6     424 non-null float64\n",
      "7     424 non-null float64\n",
      "8     424 non-null float64\n",
      "9     424 non-null float64\n",
      "10    424 non-null float64\n",
      "11    424 non-null int64\n",
      "12    424 non-null int64\n",
      "13    424 non-null float64\n",
      "14    424 non-null int64\n",
      "dtypes: float64(7), int64(7), object(1)\n",
      "memory usage: 49.8+ KB\n"
     ]
    }
   ],
   "source": [
    "cn_features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn_features = cn_features.drop([0],axis=1)\n",
    "cn_features = cn_features.drop([3],axis=1)\n",
    "cn_features = cn_features.drop([4],axis=1)\n",
    "cn_features = cn_features.drop([11],axis=1)\n",
    "cn_features = cn_features.drop([12],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(424, 10)\n"
     ]
    }
   ],
   "source": [
    "# cn_features.describe()\n",
    "print(cn_features.get_values().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.15550818e+05 2.40172000e+05 4.79132311e+04 2.29567772e+09]\n",
      " [3.52342802e+05 3.97536000e+05 8.93410266e+04 7.98181904e+09]\n",
      " [3.71293046e+00 3.72309126e+00 9.67094041e-02 9.35270884e-03]\n",
      " [5.58244223e+00 5.53273364e+00 2.06505754e-01 4.26446264e-02]\n",
      " [6.76835548e-02 6.83262730e-02 5.09073327e-03 2.59155652e-05]\n",
      " [1.57246720e-05 1.38218025e-05 3.03163104e-06 9.19078675e-12]\n",
      " [3.87060291e-04 3.38325000e-04 1.06622774e-04 1.13684159e-08]\n",
      " [2.30533224e-01 2.36405946e-01 1.56367125e-02 2.44506777e-04]\n",
      " [2.30533224e-01 2.36405946e-01 1.56367125e-02 2.44506777e-04]\n",
      " [5.28301887e-01 1.00000000e+00 5.09160477e-01 2.59244391e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Calculando as medidas de localização e dispersão\n",
    "stats = np.zeros([4,10])\n",
    "counter = 0\n",
    "for column in cn_features:\n",
    "#     print(column)\n",
    "    stats[0,counter] = cn_features[column].mean()\n",
    "    stats[1,counter] = cn_features[column].median()\n",
    "#     stats[2,counter] = cn_features[column].mode()\n",
    "    stats[2,counter] = cn_features[column].std()\n",
    "    stats[3,counter] = cn_features[column].var()\n",
    "#         stats[3,counter] = mydf[column].skew()\n",
    "#         stats[4,counter] = mydf[column].kurt()\n",
    "    counter += 1\n",
    "print(stats.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197665</td>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.793149</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.572138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214964</td>\n",
       "      <td>0.221916</td>\n",
       "      <td>0.799994</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.598173</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.459421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162983</td>\n",
       "      <td>0.185103</td>\n",
       "      <td>0.842367</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.622663</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.364839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242440</td>\n",
       "      <td>0.238390</td>\n",
       "      <td>0.886523</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.559883</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.603046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263409</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.835281</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.621519</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.668801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.197665  0.208098  0.793149  0.835600  0.606755  0.647579  0.226739   \n",
       "1  0.214964  0.221916  0.799994  0.861376  0.598173  0.620801  0.237816   \n",
       "2  0.162983  0.185103  0.842367  0.814755  0.622663  0.713141  0.248812   \n",
       "3  0.242440  0.238390  0.886523  0.809617  0.559883  0.570203  0.229935   \n",
       "4  0.263409  0.262010  0.835281  0.821811  0.621519  0.552144  0.224306   \n",
       "\n",
       "          7         8  \n",
       "0  0.572138  0.572138  \n",
       "1  0.459421  0.459421  \n",
       "2  0.364839  0.364839  \n",
       "3  0.603046  0.603046  \n",
       "4  0.668801  0.668801  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "cn_features_data = cn_features.get_values()\n",
    "# Create the Scaler object\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "# Fit your data on the scaler object\n",
    "scaled_cn = scaler.fit_transform(cn_features_data[:,0:9])\n",
    "scaled_cn = pd.DataFrame(scaled_cn)\n",
    "\n",
    "scaled_cn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197665</td>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.793149</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.572138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214964</td>\n",
       "      <td>0.221916</td>\n",
       "      <td>0.799994</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.598173</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.459421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162983</td>\n",
       "      <td>0.185103</td>\n",
       "      <td>0.842367</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.622663</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.364839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242440</td>\n",
       "      <td>0.238390</td>\n",
       "      <td>0.886523</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.559883</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.603046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263409</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.835281</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.621519</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.668801</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.197665  0.208098  0.793149  0.835600  0.606755  0.647579  0.226739   \n",
       "1  0.214964  0.221916  0.799994  0.861376  0.598173  0.620801  0.237816   \n",
       "2  0.162983  0.185103  0.842367  0.814755  0.622663  0.713141  0.248812   \n",
       "3  0.242440  0.238390  0.886523  0.809617  0.559883  0.570203  0.229935   \n",
       "4  0.263409  0.262010  0.835281  0.821811  0.621519  0.552144  0.224306   \n",
       "\n",
       "          7         8  \n",
       "0  0.572138  0.572138  \n",
       "1  0.459421  0.459421  \n",
       "2  0.364839  0.364839  \n",
       "3  0.603046  0.603046  \n",
       "4  0.668801  0.668801  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_cn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "      <td>424.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.503353</td>\n",
       "      <td>0.525324</td>\n",
       "      <td>0.882974</td>\n",
       "      <td>0.598993</td>\n",
       "      <td>0.768645</td>\n",
       "      <td>0.411453</td>\n",
       "      <td>0.137865</td>\n",
       "      <td>0.555744</td>\n",
       "      <td>0.555744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.335569</td>\n",
       "      <td>0.329114</td>\n",
       "      <td>0.083848</td>\n",
       "      <td>0.239668</td>\n",
       "      <td>0.133421</td>\n",
       "      <td>0.312091</td>\n",
       "      <td>0.114865</td>\n",
       "      <td>0.203727</td>\n",
       "      <td>0.203727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.169536</td>\n",
       "      <td>0.200874</td>\n",
       "      <td>0.824919</td>\n",
       "      <td>0.344800</td>\n",
       "      <td>0.666544</td>\n",
       "      <td>0.121707</td>\n",
       "      <td>0.035893</td>\n",
       "      <td>0.393536</td>\n",
       "      <td>0.393536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.675792</td>\n",
       "      <td>0.691806</td>\n",
       "      <td>0.891784</td>\n",
       "      <td>0.541301</td>\n",
       "      <td>0.785490</td>\n",
       "      <td>0.215562</td>\n",
       "      <td>0.085363</td>\n",
       "      <td>0.632258</td>\n",
       "      <td>0.632258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.817997</td>\n",
       "      <td>0.842372</td>\n",
       "      <td>0.950675</td>\n",
       "      <td>0.841181</td>\n",
       "      <td>0.888669</td>\n",
       "      <td>0.724542</td>\n",
       "      <td>0.229801</td>\n",
       "      <td>0.717782</td>\n",
       "      <td>0.717782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "count  424.000000  424.000000  424.000000  424.000000  424.000000  424.000000   \n",
       "mean     0.503353    0.525324    0.882974    0.598993    0.768645    0.411453   \n",
       "std      0.335569    0.329114    0.083848    0.239668    0.133421    0.312091   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.169536    0.200874    0.824919    0.344800    0.666544    0.121707   \n",
       "50%      0.675792    0.691806    0.891784    0.541301    0.785490    0.215562   \n",
       "75%      0.817997    0.842372    0.950675    0.841181    0.888669    0.724542   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "                6           7           8  \n",
       "count  424.000000  424.000000  424.000000  \n",
       "mean     0.137865    0.555744    0.555744  \n",
       "std      0.114865    0.203727    0.203727  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.035893    0.393536    0.393536  \n",
       "50%      0.085363    0.632258    0.632258  \n",
       "75%      0.229801    0.717782    0.717782  \n",
       "max      1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_cn.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cn_features = cn_features.drop([0],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "hist = scaled_cn.hist(bins=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = scaled_cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gerando o histograma\n",
    "counter = 1\n",
    "plt.figure(figsize = (7,7))\n",
    "for column in mydf:\n",
    "    plt.subplot(4,3,int(counter))\n",
    "    plt.title(column)\n",
    "    count, division = np.histogram(mydf[column])\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    plt.hist(count)\n",
    "    counter += 1\n",
    "plt.show()\n",
    "plt.savefig('hist.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 9 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gerando o boxplot\n",
    "counter = 1\n",
    "plt.figure(figsize = (10,10))\n",
    "for column in mydf:\n",
    "    plt.subplot(4,3,int(counter))\n",
    "    plt.title(column)\n",
    "    plt.subplots_adjust(hspace=0.5)\n",
    "    mydf.boxplot(column, grid=False)\n",
    "    counter += 1\n",
    "plt.show()\n",
    "plt.savefig('boxplot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "X_best=SelectKBest(f_regression, k=5)\n",
    "X_new=X_best.fit_transform(scaled_cn,cn_features[14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([0, 1, 3, 5, 7], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "mask = X_best.get_support()\n",
    "new_features = scaled_cn.columns[mask]\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_kbest = scaled_cn[[0,1,3,5,7]]\n",
    "# data_kbest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateSubDataSet(data):\n",
    "    features_to_delete = []\n",
    "    for column in data:\n",
    "        if random.randint(0, 1) == 0:  # Delete column\n",
    "            features_to_delete = np.append(features_to_delete, column)\n",
    "            \n",
    "    subdataset = data.drop(features_to_delete, 1)\n",
    "    return subdataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInconsistency(subdataset, data):    \n",
    "    features = subdataset.drop('Id', axis=1).columns\n",
    "\n",
    "#    print('features: ', features)\n",
    "    subdataset['is_duplicated'] = subdataset.duplicated(features)\n",
    "    n  = subdataset['is_duplicated'].sum()    \n",
    "#    print('n: ', n)\n",
    "    classes_n = []\n",
    "    for index, row in subdataset.iterrows():\n",
    "        if row['is_duplicated'] == True:\n",
    "            idx = row['Id']            \n",
    "            current_class = data.loc[ idx - 1 , \"Class\"]\n",
    "            classes_n = np.append(current_class, classes_n)\n",
    "    \n",
    "    unique_elements, counts_elements = np.unique(classes_n, return_counts=True)\n",
    "\n",
    "    incosistency = 0\n",
    "    for i in range (n):\n",
    "        incosistency = incosistency + (n - counts_elements[i])\n",
    "    \n",
    "    if incosistency != 0:\n",
    "        incosistency /= (n * 2)\n",
    "    \n",
    "    return incosistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LVF(data, max_tries, allow_inconsistency):\n",
    "    n_features      = data.shape[0]\n",
    "    best_n_features = n_features\n",
    "    best_subdataset = []\n",
    "    for i in range(max_tries):                    \n",
    "        current_subdataset       = generateSubDataSet(data.drop(['Class', 'Id'], axis = 1))\n",
    "        current_subdataset['Id'] = data['Id']\n",
    "        current_subdataset[0] = data[0]        \n",
    "        current_n_features = current_subdataset.shape[0]        \n",
    "        if current_n_features < best_n_features:\n",
    "            if getInconsistency(current_subdataset, data) < allow_inconsistency:\n",
    "                best_subdataset = current_subdataset\n",
    "                best_n_features = current_n_features            \n",
    "        elif current_n_features == best_n_features and getInconsistency(current_subdataset, data) < allow_inconsistency:\n",
    "            best_subdataset = current_subdataset\n",
    "    best_subdataset = best_subdataset.drop('is_duplicated',1)\n",
    "    return best_subdataset#, best_n_features      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determinateDataWithOutCorrelation(data, index, column_class):\n",
    "    corr_data = data.corr()         \n",
    "    corr_data = corr_data.where(np.triu(np.ones(corr_data.shape), k = 1).astype(np.bool))        \n",
    "    to_drop   = [column for column in corr_data.columns if any(corr_data[column].abs() > index)]\n",
    "     \n",
    "    print('FEATURES TO DELETE')\n",
    "    print(to_drop)\n",
    "    new_data  = pd.DataFrame(data = data)\n",
    "     \n",
    "    for column in new_data:\n",
    "        if column in to_drop  and column_class != column:\n",
    "            new_data = new_data.drop(column, 1)\n",
    "     \n",
    "    return new_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CSF(df):\n",
    "     # Empezamos a dibujar\n",
    "    plt.figure(figsize=(30,25))\n",
    "    \n",
    "    # Obtenemos la correlacion entre los atributos\n",
    "    cor = df.corr()\n",
    "    print('########## CORRELATION BETWEEN FEATURES ##########')\n",
    "    print(cor)\n",
    "    \n",
    "    # Ploteamos correlation\n",
    "    #sns.heatmap(cor, annot=True, cmap=plt.cm.Blues)\n",
    "    #plt.savefig('Correlations.png')\n",
    "    \n",
    "    # Correlation de todos los atributos contra el atributo 'SalePrice'\n",
    "    cor_target = abs(cor['Class'])\n",
    "    \n",
    "    # Seleccionamos los atributos con al menos 0,5 de correlacion\n",
    "    relevant_features = cor_target[cor_target>0.7]\n",
    "    \n",
    "    print('########## FEATURES WITH LONG CORRELATION WITH SALEPRICE ##########')\n",
    "    print(relevant_features)\n",
    "    \n",
    "    for column in df:\n",
    "        if column not in relevant_features:\n",
    "            df = df.drop(column, 1)\n",
    "            \n",
    "    print('########## DATASET ( FEATURES WITH LONG CORRELATION WITH SALEPRICE )  ##########')\n",
    "    print(df)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## DATASET ( FEATURES WITH LONG CORRELATION WITH SALEPRICE AND WITHOUT CORRELATION BETWEEN THEM)  ##########\n",
      "FEATURES TO DELETE\n",
      "[1, 3, 5, 6, 8]\n",
      "            0         2         4         7\n",
      "0    0.197665  0.793149  0.606755  0.572138\n",
      "1    0.214964  0.799994  0.598173  0.459421\n",
      "2    0.162983  0.842367  0.622663  0.364839\n",
      "3    0.242440  0.886523  0.559883  0.603046\n",
      "4    0.263409  0.835281  0.621519  0.668801\n",
      "5    0.233020  0.857198  0.646440  0.608888\n",
      "6    0.215286  0.927977  0.718442  0.514948\n",
      "7    0.164706  0.921957  0.697630  0.380626\n",
      "8    0.164573  0.966119  0.736253  0.262418\n",
      "9    0.160994  0.928531  0.730628  0.295717\n",
      "10   0.162843  0.924894  0.721931  0.312416\n",
      "11   0.163942  0.891487  0.695326  0.394300\n",
      "12   0.168088  0.838728  0.682743  0.434526\n",
      "13   0.062256  0.838014  0.855763  0.267378\n",
      "14   0.088246  0.844733  0.783234  0.631076\n",
      "15   0.090530  0.820601  0.774685  0.671671\n",
      "16   0.083435  0.827948  0.762939  0.629356\n",
      "17   0.089220  0.828300  0.786739  0.619741\n",
      "18   0.084303  0.834122  0.770121  0.633440\n",
      "19   0.092757  0.816812  0.767019  0.658601\n",
      "20   0.080409  0.818536  0.762782  0.614410\n",
      "21   0.088555  0.815947  0.753785  0.640438\n",
      "22   0.097022  0.798209  0.746627  0.711919\n",
      "23   0.137559  0.778983  0.679274  0.781465\n",
      "24   0.094557  0.806854  0.713428  0.612157\n",
      "25   0.117095  0.824949  0.715950  0.481487\n",
      "26   0.082945  0.864652  0.752639  0.382584\n",
      "27   0.092351  0.801485  0.721808  0.604400\n",
      "28   0.132825  0.780454  0.693771  0.516195\n",
      "29   0.163466  0.786193  0.654306  0.489414\n",
      "..        ...       ...       ...       ...\n",
      "394  0.959603  0.972489  0.922134  0.712320\n",
      "395  0.910136  0.979747  0.891372  0.617431\n",
      "396  0.940140  0.973982  0.903958  0.649055\n",
      "397  0.914513  0.975716  0.898214  0.677326\n",
      "398  0.951324  0.973606  0.928402  0.689096\n",
      "399  0.939376  0.973121  0.913243  0.635961\n",
      "400  0.947661  0.968104  0.927577  0.673878\n",
      "401  0.942794  0.962745  0.936720  0.610962\n",
      "402  0.954196  0.963190  0.952675  0.627127\n",
      "403  0.944160  0.971039  0.939346  0.621496\n",
      "404  0.943452  0.947777  0.920533  0.670910\n",
      "405  0.910633  0.969585  0.889532  0.638184\n",
      "406  0.938767  0.952605  0.904749  0.647827\n",
      "407  0.914338  0.941070  0.884430  0.624819\n",
      "408  0.951612  0.946129  0.914841  0.693145\n",
      "409  0.905541  0.924634  0.918821  0.693350\n",
      "410  0.928506  0.930753  0.928765  0.690032\n",
      "411  0.902943  0.920810  0.907397  0.675045\n",
      "412  0.932891  0.922346  0.922303  0.664000\n",
      "413  0.904876  0.930241  0.926964  0.645988\n",
      "414  0.931077  0.936625  0.952301  0.671505\n",
      "415  0.907369  0.943662  0.946736  0.619251\n",
      "416  0.933836  0.920072  0.943244  0.654453\n",
      "417  0.918288  0.953912  0.933534  0.636007\n",
      "418  0.939726  0.958624  0.927180  0.636246\n",
      "419  0.917406  0.964402  0.921168  0.642748\n",
      "420  0.948600  0.963726  0.968366  0.645120\n",
      "421  0.904561  0.958302  0.958368  0.645231\n",
      "422  0.922791  0.950686  0.963734  0.690762\n",
      "423  0.913715  0.953092  0.964965  0.659755\n",
      "\n",
      "[424 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print('########## DATASET ( FEATURES WITH LONG CORRELATION WITH SALEPRICE AND WITHOUT CORRELATION BETWEEN THEM)  ##########')\n",
    "# Eliminamos los atributos con un alto indice de correlacion\n",
    "new_df = determinateDataWithOutCorrelation(mydf, 0.85, \"Class\")\n",
    "print(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_CSF=CSF(mydf)\n",
    "#print(data_CSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_corr = mydf[[1, 3, 5, 6, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydf = scaled_cn\n",
    "col0 = np.zeros([424,1],dtype=int)\n",
    "for i in range(col0.shape[0]):\n",
    "    col0[i] = i\n",
    "col0 = pd.DataFrame(data = col0)\n",
    "col0.columns = ['Id']    \n",
    "mydf = pd.concat([mydf,col0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197665</td>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.793149</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214964</td>\n",
       "      <td>0.221916</td>\n",
       "      <td>0.799994</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.598173</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162983</td>\n",
       "      <td>0.185103</td>\n",
       "      <td>0.842367</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.622663</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242440</td>\n",
       "      <td>0.238390</td>\n",
       "      <td>0.886523</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.559883</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263409</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.835281</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.621519</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.197665  0.208098  0.793149  0.835600  0.606755  0.647579  0.226739   \n",
       "1  0.214964  0.221916  0.799994  0.861376  0.598173  0.620801  0.237816   \n",
       "2  0.162983  0.185103  0.842367  0.814755  0.622663  0.713141  0.248812   \n",
       "3  0.242440  0.238390  0.886523  0.809617  0.559883  0.570203  0.229935   \n",
       "4  0.263409  0.262010  0.835281  0.821811  0.621519  0.552144  0.224306   \n",
       "\n",
       "          7         8  Id  \n",
       "0  0.572138  0.572138   0  \n",
       "1  0.459421  0.459421   1  \n",
       "2  0.364839  0.364839   2  \n",
       "3  0.603046  0.603046   3  \n",
       "4  0.668801  0.668801   4  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =pd.DataFrame(data = cn_features.get_values()[:,9])\n",
    "label.columns = ['Class']\n",
    "mydf = pd.concat([mydf, label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Id</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197665</td>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.793149</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214964</td>\n",
       "      <td>0.221916</td>\n",
       "      <td>0.799994</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.598173</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162983</td>\n",
       "      <td>0.185103</td>\n",
       "      <td>0.842367</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.622663</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242440</td>\n",
       "      <td>0.238390</td>\n",
       "      <td>0.886523</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.559883</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263409</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.835281</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.621519</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.197665  0.208098  0.793149  0.835600  0.606755  0.647579  0.226739   \n",
       "1  0.214964  0.221916  0.799994  0.861376  0.598173  0.620801  0.237816   \n",
       "2  0.162983  0.185103  0.842367  0.814755  0.622663  0.713141  0.248812   \n",
       "3  0.242440  0.238390  0.886523  0.809617  0.559883  0.570203  0.229935   \n",
       "4  0.263409  0.262010  0.835281  0.821811  0.621519  0.552144  0.224306   \n",
       "\n",
       "          7         8  Id  Class  \n",
       "0  0.572138  0.572138   0    0.0  \n",
       "1  0.459421  0.459421   1    0.0  \n",
       "2  0.364839  0.364839   2    0.0  \n",
       "3  0.603046  0.603046   3    0.0  \n",
       "4  0.668801  0.668801   4    0.0  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos dataset\n",
    "new_df2 = LVF(mydf, 50, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>Id</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0</td>\n",
       "      <td>0.197665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.598173</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>1</td>\n",
       "      <td>0.214964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.622663</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>2</td>\n",
       "      <td>0.162983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.559883</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>3</td>\n",
       "      <td>0.242440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.621519</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>4</td>\n",
       "      <td>0.263409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.782635</td>\n",
       "      <td>0.646440</td>\n",
       "      <td>0.608479</td>\n",
       "      <td>0.205469</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>0.608888</td>\n",
       "      <td>5</td>\n",
       "      <td>0.233020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.788918</td>\n",
       "      <td>0.718442</td>\n",
       "      <td>0.649084</td>\n",
       "      <td>0.191683</td>\n",
       "      <td>0.514948</td>\n",
       "      <td>0.514948</td>\n",
       "      <td>6</td>\n",
       "      <td>0.215286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.800161</td>\n",
       "      <td>0.697630</td>\n",
       "      <td>0.727173</td>\n",
       "      <td>0.241067</td>\n",
       "      <td>0.380626</td>\n",
       "      <td>0.380626</td>\n",
       "      <td>7</td>\n",
       "      <td>0.164706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.694519</td>\n",
       "      <td>0.736253</td>\n",
       "      <td>0.731046</td>\n",
       "      <td>0.204279</td>\n",
       "      <td>0.262418</td>\n",
       "      <td>0.262418</td>\n",
       "      <td>8</td>\n",
       "      <td>0.164573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.752788</td>\n",
       "      <td>0.730628</td>\n",
       "      <td>0.739066</td>\n",
       "      <td>0.207416</td>\n",
       "      <td>0.295717</td>\n",
       "      <td>0.295717</td>\n",
       "      <td>9</td>\n",
       "      <td>0.160994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.760896</td>\n",
       "      <td>0.721931</td>\n",
       "      <td>0.733704</td>\n",
       "      <td>0.204077</td>\n",
       "      <td>0.312416</td>\n",
       "      <td>0.312416</td>\n",
       "      <td>10</td>\n",
       "      <td>0.162843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.809050</td>\n",
       "      <td>0.695326</td>\n",
       "      <td>0.734116</td>\n",
       "      <td>0.218988</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>0.394300</td>\n",
       "      <td>11</td>\n",
       "      <td>0.163942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.785306</td>\n",
       "      <td>0.682743</td>\n",
       "      <td>0.724217</td>\n",
       "      <td>0.217895</td>\n",
       "      <td>0.434526</td>\n",
       "      <td>0.434526</td>\n",
       "      <td>12</td>\n",
       "      <td>0.168088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.847604</td>\n",
       "      <td>0.855763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241215</td>\n",
       "      <td>0.267378</td>\n",
       "      <td>0.267378</td>\n",
       "      <td>13</td>\n",
       "      <td>0.062256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.852073</td>\n",
       "      <td>0.783234</td>\n",
       "      <td>0.937689</td>\n",
       "      <td>0.222194</td>\n",
       "      <td>0.631076</td>\n",
       "      <td>0.631076</td>\n",
       "      <td>14</td>\n",
       "      <td>0.088246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.865057</td>\n",
       "      <td>0.774685</td>\n",
       "      <td>0.930497</td>\n",
       "      <td>0.229855</td>\n",
       "      <td>0.671671</td>\n",
       "      <td>0.671671</td>\n",
       "      <td>15</td>\n",
       "      <td>0.090530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.829952</td>\n",
       "      <td>0.762939</td>\n",
       "      <td>0.946552</td>\n",
       "      <td>0.229516</td>\n",
       "      <td>0.629356</td>\n",
       "      <td>0.629356</td>\n",
       "      <td>16</td>\n",
       "      <td>0.083435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.843531</td>\n",
       "      <td>0.786739</td>\n",
       "      <td>0.937993</td>\n",
       "      <td>0.229796</td>\n",
       "      <td>0.619741</td>\n",
       "      <td>0.619741</td>\n",
       "      <td>17</td>\n",
       "      <td>0.089220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.855829</td>\n",
       "      <td>0.770121</td>\n",
       "      <td>0.944549</td>\n",
       "      <td>0.235665</td>\n",
       "      <td>0.633440</td>\n",
       "      <td>0.633440</td>\n",
       "      <td>18</td>\n",
       "      <td>0.084303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.863664</td>\n",
       "      <td>0.767019</td>\n",
       "      <td>0.926908</td>\n",
       "      <td>0.235073</td>\n",
       "      <td>0.658601</td>\n",
       "      <td>0.658601</td>\n",
       "      <td>19</td>\n",
       "      <td>0.092757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.844995</td>\n",
       "      <td>0.762782</td>\n",
       "      <td>0.955877</td>\n",
       "      <td>0.225039</td>\n",
       "      <td>0.614410</td>\n",
       "      <td>0.614410</td>\n",
       "      <td>20</td>\n",
       "      <td>0.080409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.835996</td>\n",
       "      <td>0.753785</td>\n",
       "      <td>0.934333</td>\n",
       "      <td>0.231301</td>\n",
       "      <td>0.640438</td>\n",
       "      <td>0.640438</td>\n",
       "      <td>21</td>\n",
       "      <td>0.088555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.841583</td>\n",
       "      <td>0.746627</td>\n",
       "      <td>0.908033</td>\n",
       "      <td>0.229000</td>\n",
       "      <td>0.711919</td>\n",
       "      <td>0.711919</td>\n",
       "      <td>22</td>\n",
       "      <td>0.097022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.846351</td>\n",
       "      <td>0.679274</td>\n",
       "      <td>0.806536</td>\n",
       "      <td>0.235264</td>\n",
       "      <td>0.781465</td>\n",
       "      <td>0.781465</td>\n",
       "      <td>23</td>\n",
       "      <td>0.137559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.852018</td>\n",
       "      <td>0.713428</td>\n",
       "      <td>0.886342</td>\n",
       "      <td>0.256421</td>\n",
       "      <td>0.612157</td>\n",
       "      <td>0.612157</td>\n",
       "      <td>24</td>\n",
       "      <td>0.094557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.821013</td>\n",
       "      <td>0.715950</td>\n",
       "      <td>0.839756</td>\n",
       "      <td>0.234634</td>\n",
       "      <td>0.481487</td>\n",
       "      <td>0.481487</td>\n",
       "      <td>25</td>\n",
       "      <td>0.117095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.841070</td>\n",
       "      <td>0.752639</td>\n",
       "      <td>0.924963</td>\n",
       "      <td>0.236110</td>\n",
       "      <td>0.382584</td>\n",
       "      <td>0.382584</td>\n",
       "      <td>26</td>\n",
       "      <td>0.082945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.851018</td>\n",
       "      <td>0.721808</td>\n",
       "      <td>0.876418</td>\n",
       "      <td>0.244063</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>0.604400</td>\n",
       "      <td>27</td>\n",
       "      <td>0.092351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.793699</td>\n",
       "      <td>0.693771</td>\n",
       "      <td>0.775873</td>\n",
       "      <td>0.250092</td>\n",
       "      <td>0.516195</td>\n",
       "      <td>0.516195</td>\n",
       "      <td>28</td>\n",
       "      <td>0.132825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.777048</td>\n",
       "      <td>0.654306</td>\n",
       "      <td>0.715174</td>\n",
       "      <td>0.240381</td>\n",
       "      <td>0.489414</td>\n",
       "      <td>0.489414</td>\n",
       "      <td>29</td>\n",
       "      <td>0.163466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0.331155</td>\n",
       "      <td>0.922134</td>\n",
       "      <td>0.024464</td>\n",
       "      <td>0.008276</td>\n",
       "      <td>0.712320</td>\n",
       "      <td>0.712320</td>\n",
       "      <td>394</td>\n",
       "      <td>0.959603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0.302813</td>\n",
       "      <td>0.891372</td>\n",
       "      <td>0.057657</td>\n",
       "      <td>0.041876</td>\n",
       "      <td>0.617431</td>\n",
       "      <td>0.617431</td>\n",
       "      <td>395</td>\n",
       "      <td>0.910136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0.314833</td>\n",
       "      <td>0.903958</td>\n",
       "      <td>0.038620</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.649055</td>\n",
       "      <td>0.649055</td>\n",
       "      <td>396</td>\n",
       "      <td>0.940140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0.332746</td>\n",
       "      <td>0.898214</td>\n",
       "      <td>0.055286</td>\n",
       "      <td>0.028857</td>\n",
       "      <td>0.677326</td>\n",
       "      <td>0.677326</td>\n",
       "      <td>397</td>\n",
       "      <td>0.914513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>0.298857</td>\n",
       "      <td>0.928402</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>0.009815</td>\n",
       "      <td>0.689096</td>\n",
       "      <td>0.689096</td>\n",
       "      <td>398</td>\n",
       "      <td>0.951324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.353037</td>\n",
       "      <td>0.913243</td>\n",
       "      <td>0.039094</td>\n",
       "      <td>0.020986</td>\n",
       "      <td>0.635961</td>\n",
       "      <td>0.635961</td>\n",
       "      <td>399</td>\n",
       "      <td>0.939376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.358829</td>\n",
       "      <td>0.927577</td>\n",
       "      <td>0.032546</td>\n",
       "      <td>0.018732</td>\n",
       "      <td>0.673878</td>\n",
       "      <td>0.673878</td>\n",
       "      <td>400</td>\n",
       "      <td>0.947661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>0.305313</td>\n",
       "      <td>0.936720</td>\n",
       "      <td>0.035657</td>\n",
       "      <td>0.012584</td>\n",
       "      <td>0.610962</td>\n",
       "      <td>0.610962</td>\n",
       "      <td>401</td>\n",
       "      <td>0.942794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>0.310021</td>\n",
       "      <td>0.952675</td>\n",
       "      <td>0.030172</td>\n",
       "      <td>0.009587</td>\n",
       "      <td>0.627127</td>\n",
       "      <td>0.627127</td>\n",
       "      <td>402</td>\n",
       "      <td>0.954196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0.346834</td>\n",
       "      <td>0.939346</td>\n",
       "      <td>0.044425</td>\n",
       "      <td>0.011652</td>\n",
       "      <td>0.621496</td>\n",
       "      <td>0.621496</td>\n",
       "      <td>403</td>\n",
       "      <td>0.944160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>0.309768</td>\n",
       "      <td>0.920533</td>\n",
       "      <td>0.040113</td>\n",
       "      <td>0.013326</td>\n",
       "      <td>0.670910</td>\n",
       "      <td>0.670910</td>\n",
       "      <td>404</td>\n",
       "      <td>0.943452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>0.322365</td>\n",
       "      <td>0.889532</td>\n",
       "      <td>0.062591</td>\n",
       "      <td>0.027060</td>\n",
       "      <td>0.638184</td>\n",
       "      <td>0.638184</td>\n",
       "      <td>405</td>\n",
       "      <td>0.910633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>0.332280</td>\n",
       "      <td>0.904749</td>\n",
       "      <td>0.046306</td>\n",
       "      <td>0.017196</td>\n",
       "      <td>0.647827</td>\n",
       "      <td>0.647827</td>\n",
       "      <td>406</td>\n",
       "      <td>0.938767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>407</th>\n",
       "      <td>0.334317</td>\n",
       "      <td>0.884430</td>\n",
       "      <td>0.059659</td>\n",
       "      <td>0.011898</td>\n",
       "      <td>0.624819</td>\n",
       "      <td>0.624819</td>\n",
       "      <td>407</td>\n",
       "      <td>0.914338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>408</th>\n",
       "      <td>0.390176</td>\n",
       "      <td>0.914841</td>\n",
       "      <td>0.033252</td>\n",
       "      <td>0.021072</td>\n",
       "      <td>0.693145</td>\n",
       "      <td>0.693145</td>\n",
       "      <td>408</td>\n",
       "      <td>0.951612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>0.367956</td>\n",
       "      <td>0.918821</td>\n",
       "      <td>0.067808</td>\n",
       "      <td>0.021093</td>\n",
       "      <td>0.693350</td>\n",
       "      <td>0.693350</td>\n",
       "      <td>409</td>\n",
       "      <td>0.905541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>0.345097</td>\n",
       "      <td>0.928765</td>\n",
       "      <td>0.050768</td>\n",
       "      <td>0.014391</td>\n",
       "      <td>0.690032</td>\n",
       "      <td>0.690032</td>\n",
       "      <td>410</td>\n",
       "      <td>0.928506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>0.345058</td>\n",
       "      <td>0.907397</td>\n",
       "      <td>0.065059</td>\n",
       "      <td>0.021685</td>\n",
       "      <td>0.675045</td>\n",
       "      <td>0.675045</td>\n",
       "      <td>411</td>\n",
       "      <td>0.902943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>0.353558</td>\n",
       "      <td>0.922303</td>\n",
       "      <td>0.044872</td>\n",
       "      <td>0.019760</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>0.664000</td>\n",
       "      <td>412</td>\n",
       "      <td>0.932891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>0.280940</td>\n",
       "      <td>0.926964</td>\n",
       "      <td>0.064665</td>\n",
       "      <td>0.016489</td>\n",
       "      <td>0.645988</td>\n",
       "      <td>0.645988</td>\n",
       "      <td>413</td>\n",
       "      <td>0.904876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>0.334873</td>\n",
       "      <td>0.952301</td>\n",
       "      <td>0.051131</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.671505</td>\n",
       "      <td>0.671505</td>\n",
       "      <td>414</td>\n",
       "      <td>0.931077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>0.322464</td>\n",
       "      <td>0.946736</td>\n",
       "      <td>0.067089</td>\n",
       "      <td>0.012874</td>\n",
       "      <td>0.619251</td>\n",
       "      <td>0.619251</td>\n",
       "      <td>415</td>\n",
       "      <td>0.907369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>0.322883</td>\n",
       "      <td>0.943244</td>\n",
       "      <td>0.046998</td>\n",
       "      <td>0.008689</td>\n",
       "      <td>0.654453</td>\n",
       "      <td>0.654453</td>\n",
       "      <td>416</td>\n",
       "      <td>0.933836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.342402</td>\n",
       "      <td>0.933534</td>\n",
       "      <td>0.056549</td>\n",
       "      <td>0.023903</td>\n",
       "      <td>0.636007</td>\n",
       "      <td>0.636007</td>\n",
       "      <td>417</td>\n",
       "      <td>0.918288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>0.319387</td>\n",
       "      <td>0.927180</td>\n",
       "      <td>0.039880</td>\n",
       "      <td>0.008803</td>\n",
       "      <td>0.636246</td>\n",
       "      <td>0.636246</td>\n",
       "      <td>418</td>\n",
       "      <td>0.939726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419</th>\n",
       "      <td>0.333051</td>\n",
       "      <td>0.921168</td>\n",
       "      <td>0.053580</td>\n",
       "      <td>0.018394</td>\n",
       "      <td>0.642748</td>\n",
       "      <td>0.642748</td>\n",
       "      <td>419</td>\n",
       "      <td>0.917406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>0.327433</td>\n",
       "      <td>0.968366</td>\n",
       "      <td>0.032384</td>\n",
       "      <td>0.003976</td>\n",
       "      <td>0.645120</td>\n",
       "      <td>0.645120</td>\n",
       "      <td>420</td>\n",
       "      <td>0.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>0.339288</td>\n",
       "      <td>0.958368</td>\n",
       "      <td>0.063157</td>\n",
       "      <td>0.022509</td>\n",
       "      <td>0.645231</td>\n",
       "      <td>0.645231</td>\n",
       "      <td>421</td>\n",
       "      <td>0.904561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422</th>\n",
       "      <td>0.350847</td>\n",
       "      <td>0.963734</td>\n",
       "      <td>0.049429</td>\n",
       "      <td>0.013712</td>\n",
       "      <td>0.690762</td>\n",
       "      <td>0.690762</td>\n",
       "      <td>422</td>\n",
       "      <td>0.922791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>0.355998</td>\n",
       "      <td>0.964965</td>\n",
       "      <td>0.055485</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>0.659755</td>\n",
       "      <td>0.659755</td>\n",
       "      <td>423</td>\n",
       "      <td>0.913715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>424 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            3         4         5         6         7         8   Id         0\n",
       "0    0.835600  0.606755  0.647579  0.226739  0.572138  0.572138    0  0.197665\n",
       "1    0.861376  0.598173  0.620801  0.237816  0.459421  0.459421    1  0.214964\n",
       "2    0.814755  0.622663  0.713141  0.248812  0.364839  0.364839    2  0.162983\n",
       "3    0.809617  0.559883  0.570203  0.229935  0.603046  0.603046    3  0.242440\n",
       "4    0.821811  0.621519  0.552144  0.224306  0.668801  0.668801    4  0.263409\n",
       "5    0.782635  0.646440  0.608479  0.205469  0.608888  0.608888    5  0.233020\n",
       "6    0.788918  0.718442  0.649084  0.191683  0.514948  0.514948    6  0.215286\n",
       "7    0.800161  0.697630  0.727173  0.241067  0.380626  0.380626    7  0.164706\n",
       "8    0.694519  0.736253  0.731046  0.204279  0.262418  0.262418    8  0.164573\n",
       "9    0.752788  0.730628  0.739066  0.207416  0.295717  0.295717    9  0.160994\n",
       "10   0.760896  0.721931  0.733704  0.204077  0.312416  0.312416   10  0.162843\n",
       "11   0.809050  0.695326  0.734116  0.218988  0.394300  0.394300   11  0.163942\n",
       "12   0.785306  0.682743  0.724217  0.217895  0.434526  0.434526   12  0.168088\n",
       "13   0.847604  0.855763  1.000000  0.241215  0.267378  0.267378   13  0.062256\n",
       "14   0.852073  0.783234  0.937689  0.222194  0.631076  0.631076   14  0.088246\n",
       "15   0.865057  0.774685  0.930497  0.229855  0.671671  0.671671   15  0.090530\n",
       "16   0.829952  0.762939  0.946552  0.229516  0.629356  0.629356   16  0.083435\n",
       "17   0.843531  0.786739  0.937993  0.229796  0.619741  0.619741   17  0.089220\n",
       "18   0.855829  0.770121  0.944549  0.235665  0.633440  0.633440   18  0.084303\n",
       "19   0.863664  0.767019  0.926908  0.235073  0.658601  0.658601   19  0.092757\n",
       "20   0.844995  0.762782  0.955877  0.225039  0.614410  0.614410   20  0.080409\n",
       "21   0.835996  0.753785  0.934333  0.231301  0.640438  0.640438   21  0.088555\n",
       "22   0.841583  0.746627  0.908033  0.229000  0.711919  0.711919   22  0.097022\n",
       "23   0.846351  0.679274  0.806536  0.235264  0.781465  0.781465   23  0.137559\n",
       "24   0.852018  0.713428  0.886342  0.256421  0.612157  0.612157   24  0.094557\n",
       "25   0.821013  0.715950  0.839756  0.234634  0.481487  0.481487   25  0.117095\n",
       "26   0.841070  0.752639  0.924963  0.236110  0.382584  0.382584   26  0.082945\n",
       "27   0.851018  0.721808  0.876418  0.244063  0.604400  0.604400   27  0.092351\n",
       "28   0.793699  0.693771  0.775873  0.250092  0.516195  0.516195   28  0.132825\n",
       "29   0.777048  0.654306  0.715174  0.240381  0.489414  0.489414   29  0.163466\n",
       "..        ...       ...       ...       ...       ...       ...  ...       ...\n",
       "394  0.331155  0.922134  0.024464  0.008276  0.712320  0.712320  394  0.959603\n",
       "395  0.302813  0.891372  0.057657  0.041876  0.617431  0.617431  395  0.910136\n",
       "396  0.314833  0.903958  0.038620  0.024330  0.649055  0.649055  396  0.940140\n",
       "397  0.332746  0.898214  0.055286  0.028857  0.677326  0.677326  397  0.914513\n",
       "398  0.298857  0.928402  0.031468  0.009815  0.689096  0.689096  398  0.951324\n",
       "399  0.353037  0.913243  0.039094  0.020986  0.635961  0.635961  399  0.939376\n",
       "400  0.358829  0.927577  0.032546  0.018732  0.673878  0.673878  400  0.947661\n",
       "401  0.305313  0.936720  0.035657  0.012584  0.610962  0.610962  401  0.942794\n",
       "402  0.310021  0.952675  0.030172  0.009587  0.627127  0.627127  402  0.954196\n",
       "403  0.346834  0.939346  0.044425  0.011652  0.621496  0.621496  403  0.944160\n",
       "404  0.309768  0.920533  0.040113  0.013326  0.670910  0.670910  404  0.943452\n",
       "405  0.322365  0.889532  0.062591  0.027060  0.638184  0.638184  405  0.910633\n",
       "406  0.332280  0.904749  0.046306  0.017196  0.647827  0.647827  406  0.938767\n",
       "407  0.334317  0.884430  0.059659  0.011898  0.624819  0.624819  407  0.914338\n",
       "408  0.390176  0.914841  0.033252  0.021072  0.693145  0.693145  408  0.951612\n",
       "409  0.367956  0.918821  0.067808  0.021093  0.693350  0.693350  409  0.905541\n",
       "410  0.345097  0.928765  0.050768  0.014391  0.690032  0.690032  410  0.928506\n",
       "411  0.345058  0.907397  0.065059  0.021685  0.675045  0.675045  411  0.902943\n",
       "412  0.353558  0.922303  0.044872  0.019760  0.664000  0.664000  412  0.932891\n",
       "413  0.280940  0.926964  0.064665  0.016489  0.645988  0.645988  413  0.904876\n",
       "414  0.334873  0.952301  0.051131  0.007017  0.671505  0.671505  414  0.931077\n",
       "415  0.322464  0.946736  0.067089  0.012874  0.619251  0.619251  415  0.907369\n",
       "416  0.322883  0.943244  0.046998  0.008689  0.654453  0.654453  416  0.933836\n",
       "417  0.342402  0.933534  0.056549  0.023903  0.636007  0.636007  417  0.918288\n",
       "418  0.319387  0.927180  0.039880  0.008803  0.636246  0.636246  418  0.939726\n",
       "419  0.333051  0.921168  0.053580  0.018394  0.642748  0.642748  419  0.917406\n",
       "420  0.327433  0.968366  0.032384  0.003976  0.645120  0.645120  420  0.948600\n",
       "421  0.339288  0.958368  0.063157  0.022509  0.645231  0.645231  421  0.904561\n",
       "422  0.350847  0.963734  0.049429  0.013712  0.690762  0.690762  422  0.922791\n",
       "423  0.355998  0.964965  0.055485  0.011636  0.659755  0.659755  423  0.913715\n",
       "\n",
       "[424 rows x 8 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lvf = mydf[[0,3,5,6,7]]\n",
    "# data_lvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df2 = new_df2.drop(['Id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.606755</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.197665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.598173</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.214964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.622663</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.162983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.559883</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.242440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.621519</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.263409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          3         4         5         6         7         8         0\n",
       "0  0.835600  0.606755  0.647579  0.226739  0.572138  0.572138  0.197665\n",
       "1  0.861376  0.598173  0.620801  0.237816  0.459421  0.459421  0.214964\n",
       "2  0.814755  0.622663  0.713141  0.248812  0.364839  0.364839  0.162983\n",
       "3  0.809617  0.559883  0.570203  0.229935  0.603046  0.603046  0.242440\n",
       "4  0.821811  0.621519  0.552144  0.224306  0.668801  0.668801  0.263409"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 'Id', 'Class']\n"
     ]
    }
   ],
   "source": [
    "##Wrapper Selection Feature\n",
    "##coded by jecs89\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import random\n",
    "\n",
    "my_list = []\n",
    "for column in mydf:\n",
    "    my_list.append(column)\n",
    "print(my_list)\n",
    "\n",
    "def my_wrapper( mydf, N_test, size_col ):\n",
    "\n",
    "    size_subset = int(mydf.shape[0]/10)\n",
    "    x_subset = np.zeros([size_subset,size_col+1])\n",
    "\n",
    "    mydf_data = mydf.get_values()\n",
    "\n",
    "    vec_row = np.zeros([ N_test, size_subset])\n",
    "    vec_col = np.zeros([ N_test, size_col ])\n",
    "    vec_diff = np.zeros([ 1, N_test ])\n",
    "\n",
    "    for n_test in range(0,N_test):\n",
    "        x_row = random.sample(range(1, mydf.shape[0]), size_subset)\n",
    "        x_row = np.sort(x_row)\n",
    "        vec_row[n_test,:] = x_row\n",
    "\n",
    "        x_col = random.sample(range(0, mydf_data.shape[1]-1), size_col)\n",
    "        x_col = np.sort(x_col)\n",
    "        vec_col[n_test,:] = x_col\n",
    "\n",
    "        c = 0\n",
    "        for idj in range(0, mydf_data.shape[1]-1):\n",
    "            for idx_subset in range(0,size_subset):\n",
    "                if( idj == x_col[c] ):\n",
    "                    x_subset[idx_subset,c] = mydf_data[x_row[idx_subset],x_col[c]]\n",
    "            if(idj == x_col[c]):\n",
    "                c += 1\n",
    "            if( c == size_col ):\n",
    "                break;\n",
    "\n",
    "        for idx_subset in range(0,size_subset):\n",
    "            x_subset[idx_subset,x_subset.shape[1]-1] = mydf_data[x_row[idx_subset],mydf_data.shape[1]-1]\n",
    "\n",
    "        x_st = x_subset[:,0:x_subset.shape[1]-1]\n",
    "        y_st = x_subset[:,x_subset.shape[1]-1]\n",
    "\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', solver='adam', alpha=0.0001).fit(x_st,y_st)\n",
    "        y_pdt = clf.predict(x_st)\n",
    "        \n",
    "        counter = 0\n",
    "        for i in range(0,x_st.shape[0]):\n",
    "            counter += (y_st[i] - y_pdt[i])*(y_st[i] - y_pdt[i])\n",
    "\n",
    "        vec_diff[0,n_test] = counter\n",
    "#         print(str(clf.score(x_st,y_st))+ ' ' + str(counter))\n",
    "\n",
    "    my_min = vec_diff[0,0]\n",
    "    idx = 0\n",
    "    for i in range(1,N_test):\n",
    "        if( my_min > vec_diff[0,i]):\n",
    "            my_min = vec_diff[0,i]\n",
    "            idx = i\n",
    "            \n",
    "#     print(vec_row[idx,:])\n",
    "#     print(vec_col)\n",
    "    \n",
    "    res = np.zeros([mydf_data.shape[0],size_col+1])\n",
    "    for i in range(0,mydf_data.shape[0]):\n",
    "        for j in range(0,size_col):\n",
    "            res[i,j] = mydf_data[i,int(vec_col[idx,j])]\n",
    "    \n",
    "    res[:,size_col] = mydf_data[:,mydf_data.shape[1]-1]\n",
    "    \n",
    "#     for i in range(0, size_col):\n",
    "    ix = 0\n",
    "    for i in range(0,size_col):\n",
    "        print( my_list[int(vec_col[idx,i])])\n",
    "        \n",
    "#     return vec_col[idx,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "size_col = 3\n",
    "N_test = 20\n",
    "mydf = pd.DataFrame(mydf)\n",
    "my_wrapper( mydf=mydf, N_test=N_test, size_col=size_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df3 = mydf[[1,2,5,'Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrapper = mydf[[1,2,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian( x1, x2):\n",
    "    count = 0\n",
    "    for i in range( 0, x1.shape[0]):\n",
    "        count += math.pow(x1[i] - x2[i],2)\n",
    "    return np.sqrt(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    counter = 0\n",
    "    num = List[0] \n",
    "      \n",
    "    for i in List: \n",
    "        curr_frequency = List.count(i) \n",
    "        if(curr_frequency> counter): \n",
    "            counter = curr_frequency \n",
    "            num = i \n",
    "  \n",
    "    return num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-NN\n",
    "# k = Quantidade de vecinos\n",
    "# x = exemplo de prediçao\n",
    "# X = dataset com somente os atributos\n",
    "# Y = as classes de predição\n",
    "def knn(K, x, X, Y,function):\n",
    "    # Calculando distancias con otros puntos\n",
    "    dist = []\n",
    "    for i in range(X.shape[0]):\n",
    "        dist.append( [ i, function( x, X[i,:] ) ] )\n",
    "    \n",
    "    # Ordenando dist crecientemente\n",
    "    dist = np.array(dist)\n",
    "    dist = dist[dist[:,1].argsort()]\n",
    "    \n",
    "    res = []\n",
    "    idx=np.zeros(K)\n",
    "    for r in range(K):\n",
    "        res.append(Y[int(dist[r,0])])\n",
    "        idx[r]=dist[r,0]\n",
    "    return res, dist,idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean Distancefrom sklearn.metrics import confusion_matrix\n",
    "\n",
    "def save_value(x,y):\n",
    "    if y == 0:\n",
    "        return 0\n",
    "    return x / y\n",
    "\n",
    "def ConfusionMatrix(y_true,y_predict):\n",
    "    CM=confusion_matrix(y_true, y_pred)\n",
    "    return CM\n",
    "\n",
    "# Multiclass Confusion Matrix\n",
    "# Entries:\n",
    "# y_true: true values of the classification\n",
    "# y_predict: predict values of the classification\n",
    "# C: quantity of classes \n",
    "\n",
    "# Haciendo la matriz de confusion binaria para la clase i\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def MultiClassConfusionMatrix(y_true,y_pred):\n",
    "    \n",
    "    C= np.unique(y_true)\n",
    "    D=len(C)\n",
    "    \n",
    "    # Matriz de confusion general \n",
    "    CM=confusion_matrix(y_true, y_pred)\n",
    "    #print('###### General Confusion Matrix #####')\n",
    "    #print(CM)\n",
    "        \n",
    "    accuracy=np.zeros(D)\n",
    "    precision=np.zeros(D)\n",
    "    recall=np.zeros(D)\n",
    "    specificity=np.zeros(D)\n",
    "    \n",
    "    \n",
    "    for i in range(D):\n",
    "        #atrib=np.array(C)\n",
    "        #print('aquiii')\n",
    "        #print(C)\n",
    "        atributo=C[i]\n",
    "        row_i=CM[i,:]\n",
    "        col_i=CM[:,i]\n",
    "        \n",
    "        row_i_without_i=np.delete(row_i,i,0)\n",
    "        #print(row_i_without_i)\n",
    "        col_i_without_i=np.delete(col_i,i,0)\n",
    "        del_row_i=np.delete(CM,i,0)\n",
    "        del_col_i=np.delete(del_row_i,i,1)\n",
    "        \n",
    "        VP=CM[i,i]\n",
    "        #print(VP)\n",
    "        FN=np.sum(row_i_without_i)\n",
    "        #print(VN)\n",
    "        FP=np.sum(col_i_without_i)\n",
    "        VN=np.sum(del_col_i)\n",
    "#         print('VP VN FP FN', VP, VN,FP,FN )\n",
    "        \n",
    "        CM_new=[[VP,FN],[FP,VN]]\n",
    "        #print(CM_new)\n",
    "        CM_new=np.array(CM_new) # casting\n",
    "\n",
    "        # calculando las medidas de desempenho\n",
    "        div1=VP+VN+FP+FN\n",
    "        #print(div1)\n",
    "        div2=VP+FP\n",
    "        div3=VP+FN\n",
    "        div4=VN+FP\n",
    "        \n",
    "        accuracy[i]=save_value((VP+VN),div1)\n",
    "        precision[i]=save_value(VP,div2)\n",
    "        recall[i]=save_value(VP,div3)\n",
    "        specificity[i]=save_value(VN,div4)\n",
    "        \n",
    "        #print('###### Confusion Matrix para clase ',atributo, ' #####')\n",
    "        #print(CM_new)    \n",
    "        \n",
    "    #Table = {'accuracy': accuracy} # 'precision': precision, 'recall': recall, 'specificity': specificity}\n",
    "    #df = pd.DataFrame(data=Table)\n",
    "    #print(type(accuracy))\n",
    "    #print(accuracy.shape)\n",
    "    return accuracy\n",
    "    #print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean Distancedef randomSubSampling(test_size,function,iris):    \n",
    "    #iris = pd.read_csv('../input/bezdekIris.data',header=None)\n",
    "    iris_data = iris.values\n",
    "    nc = len(iris.columns)\n",
    "    n_times_random_subsampling = 10\n",
    "    \n",
    "    lista=[]\n",
    "    for i in range(n_times_random_subsampling):\n",
    "        # iris_test_data é dataset con que eu vou fazer a teste\n",
    "        iris_test = iris.sample(n = test_size, replace=True)\n",
    "        iris_test_data = iris_test.values\n",
    "        \n",
    "        iris_test_data[iris_test_data.shape[0] - 2, : ] = iris_data[202, :]\n",
    "        iris_test_data[iris_test_data.shape[0] - 1, : ] = iris_data[203, :]\n",
    "\n",
    "        # iris_train_data é dataset onde eu vou fazer a busqueda de distancia\n",
    "        iris_train = iris.drop(iris_test.index.values)\n",
    "        iris_train_data = iris_train.values\n",
    "        \n",
    "        iris_train_data[iris_train_data.shape[0] - 2, : ] = iris_data[202, :]\n",
    "        iris_train_data[iris_train_data.shape[0] - 1, : ] = iris_data[203, :]\n",
    "\n",
    "        # Se guardan los atributos en X\n",
    "        X_train = iris_train_data[:,0:nc-2]\n",
    "\n",
    "        # Se guardan las clases en Y\n",
    "        Y_train = iris_train_data[:,nc-1]\n",
    "\n",
    "        K = 5\n",
    "        X_test = iris_test_data[:,0:nc-2]\n",
    "        Y_test =iris_test_data[:,nc-1]\n",
    "        Y_pred = []\n",
    "        #print('############################# ', i+1 , '-vez #############################')\n",
    "        for j in range(iris_test_data.shape[0]):\n",
    "            x = X_test[j,:]    \n",
    "            res, dist,idx = knn(K, x, X_train, Y_train,function)                \n",
    "            Y_pred.append(most_frequent(res))\n",
    "                    \n",
    "        #print('GENERAL CONFUSION MATRIX')\n",
    "        #print(confusion_matrix(Y_test, Y_pred)) \n",
    "        #print('True Value: ', Y_test)\n",
    "        #print('Predict Value: ', Y_pred)\n",
    "        \n",
    "        accuracy = MultiClassConfusionMatrix(Y_test,Y_pred)\n",
    "        lista.append(accuracy)\n",
    "    \n",
    "#     print('lista')\n",
    "#     print(lista)\n",
    "#     print(len(lista))\n",
    "    \n",
    "#     print('list elem')\n",
    "#     for l in lista:\n",
    "#         print(l)\n",
    "    \n",
    "    new_lista = np.zeros([len(lista), 3])\n",
    "    idx = 0\n",
    "    for k in lista:\n",
    "        new_lista[idx, :] = k\n",
    "        idx += 1\n",
    "    \n",
    "# #     lista = np.array(lista)\n",
    "#     print('lista')\n",
    "#     print(lista)\n",
    "#     print(lista.shape)\n",
    "    \n",
    "    M=np.sum(lista,axis=0)\n",
    "    prom=(1/n_times_random_subsampling)*M\n",
    "#     print('prom')\n",
    "#     print(prom)\n",
    "    return prom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for d in distances:\n",
    "        \n",
    "#     print('Subsampling')\n",
    "#     test_size = np.floor(0.25*424)\n",
    "#     test_size=int(test_size)\n",
    "#     prom = randomSubSampling(test_size,d,data_kbest)\n",
    "#     print(prom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98962264, 1.        , 0.98962264])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = np.floor(0.25*424)\n",
    "test_size=int(test_size)\n",
    "randomSubSampling(test_size,euclidian,new_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kFoldCrossValidation(k_value,function,iris):\n",
    "    #iris = pd.read_csv('../input/bezdekIris.data')\n",
    "    iris_data = iris.values      \n",
    "    n_times = math.ceil(iris_data.shape[0]/k_value)\n",
    "    nc=len(iris.columns)\n",
    "    \n",
    "    lista = []\n",
    "    for i in range(k_value):\n",
    "        idx = n_times * i            \n",
    "        # iris_test_data é dataset con que eu vou fazer a teste   \n",
    "        iris_test = iris.iloc[idx:idx+n_times,:]\n",
    "        iris_test_data = iris_test.values\n",
    "        \n",
    "        iris_test_data[iris_test_data.shape[0] - 4, : ] = iris_data[1, :]\n",
    "        iris_test_data[iris_test_data.shape[0] - 3, : ] = iris_data[416, :]\n",
    "        \n",
    "        iris_test_data[iris_test_data.shape[0] - 2, : ] = iris_data[202, :]\n",
    "        iris_test_data[iris_test_data.shape[0] - 1, : ] = iris_data[203, :]\n",
    "        \n",
    "        # iris_train_data é dataset onde eu vou fazer a busqueda de distancia\n",
    "        iris_train = iris.drop(iris_test.index.values)\n",
    "        iris_train_data = iris_train.values\n",
    "        \n",
    "        iris_train_data[iris_train_data.shape[0] - 4, : ] = iris_data[1, :]\n",
    "        iris_train_data[iris_train_data.shape[0] - 3, : ] = iris_data[416, :]\n",
    "        \n",
    "        iris_train_data[iris_train_data.shape[0] - 2, : ] = iris_data[202, :]\n",
    "        iris_train_data[iris_train_data.shape[0] - 1, : ] = iris_data[203, :]\n",
    "                        \n",
    "        # Se guardan los atributos en X\n",
    "        X_train = iris_train_data[:,0:nc-2]\n",
    "    \n",
    "        # Se guardan las clases en Y\n",
    "        Y_train = iris_train_data[:,nc-1]\n",
    "    \n",
    "        K = 5\n",
    "        X_test = iris_test_data[:,0:nc-2]\n",
    "        Y_test =iris_test_data[:,nc-1]\n",
    "        Y_pred = []\n",
    "#         print('############################# ', i+1 , '-vez #############################')\n",
    "        for j in range(iris_test_data.shape[0]):\n",
    "            x = X_test[j,:]    \n",
    "            res, dist,idx = knn(K, x, X_train, Y_train,function)                \n",
    "            Y_pred.append(most_frequent(res))\n",
    "         \n",
    "        #print('True Value: ', Y_test)\n",
    "        #print('Predict Value: ', Y_pred)\n",
    "        #print('CONFUSION MATRIX')\n",
    "        #print(confusion_matrix(Y_test, Y_pred)) \n",
    "#         MultiClassConfusionMatrix(Y_test,Y_pred)\n",
    "        \n",
    "        accuracy = MultiClassConfusionMatrix(Y_test,Y_pred)\n",
    "        lista.append(accuracy)\n",
    "    \n",
    "#     print('lista')\n",
    "#     print(lista)\n",
    "#     print(len(lista))\n",
    "    \n",
    "#     print('list elem')\n",
    "#     for l in lista:\n",
    "#         print(l)\n",
    "    \n",
    "#     new_lista = np.zeros([len(lista), 3])\n",
    "#     idx = 0\n",
    "#     for k in lista:\n",
    "#         new_lista[idx, :] = k\n",
    "#         idx += 1\n",
    "    \n",
    "# #     lista = np.array(lista)\n",
    "#     print('lista')\n",
    "#     print(lista)\n",
    "#     print(lista.shape)\n",
    "    \n",
    "    M=np.sum(lista,axis=0)\n",
    "    prom=(1/k_value)*M\n",
    "#     print('prom')\n",
    "#     print(prom)\n",
    "    return prom\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaveOneOut(function,iris):\n",
    "    #iris = pd.read_csv('../input/bezdekIris.data')\n",
    "    nc=len(iris.columns)\n",
    "    iris_data = iris.values\n",
    "    k_value = iris_data.shape[0]\n",
    "    kFoldCrossValidation(k_value,function,iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leaveOneOut(euclidian,new_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance 1: MSE distance \n",
    "# Obtiene distancia (MSE: Error cuadrático medio) entre dos puntos x1 y x2\n",
    "def my_dis1( x1, x2):\n",
    "    count = 0\n",
    "    for i in range( 0, x1.shape[0]):\n",
    "        count += (x1[i] - x2[i])*(x1[i] - x2[i])\n",
    "    return np.sqrt(count)\n",
    "\n",
    "\n",
    "# Distance 2: chebyshev distance (vista en clase)\n",
    "# Calcula el máx de las distancias entre las componentes de los elementos x1 y x2\n",
    "def my_dis2( x1, x2 ):\n",
    "    my_list = []\n",
    "    for i in range(0,x1.shape[0]):\n",
    "        my_list.append( np.abs( x1[i] - x2[i]))\n",
    "    my_list = np.array(my_list)\n",
    "    \n",
    "    return np.max(my_list)\n",
    "\n",
    "# Distance 3: cosine distance (no vista en clase)\n",
    "# calcula el coseno del ángulo entre los elementos n-dimensionales x1, x2\n",
    "def my_dis3( x1, x2 ):\n",
    "    my_list = []\n",
    "\n",
    "    dot = np.dot(x1, x2)\n",
    "    norma = np.linalg.norm(x1)\n",
    "    normb = np.linalg.norm(x2)\n",
    "    cos = dot / (norma * normb)\n",
    "    \n",
    "    return cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data=new_df3.get_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Euclidean Distance##My KNN Algorithm\n",
    "def my_KNN(X,Y,x, K, function):\n",
    "#     print(X[45:50,:])\n",
    "#     print(Y[45:50])\n",
    "\n",
    "    set_class = {}\n",
    "    set_class = set(set_class)\n",
    "\n",
    "    for i in range(Y.shape[0]):\n",
    "        set_class.add(Y[i])\n",
    "\n",
    "#     print(set_class)\n",
    "    \n",
    "    dist = []\n",
    "\n",
    "    for i in range(X.shape[0]):\n",
    "        dist.append( [ i, function( x, X[i,:] ) ] )\n",
    "    dist = np.array(dist)    \n",
    "    dist = dist[dist[:,1].argsort()]\n",
    "#     print(dist[0:K,0])\n",
    "\n",
    "    res = []\n",
    "    for r in range(K):\n",
    "        res.append(Y[int(dist[r,0])])\n",
    "    res = np.array(res)\n",
    "#     print(res)\n",
    "\n",
    "    count_set = np.zeros( [ 1, 3] )\n",
    "    cnt = 0\n",
    "\n",
    "#     print(set_class)\n",
    "    for i in set_class:\n",
    "        for r in range(0,K):\n",
    "#             print( i + ' ' + str(r) + ' ' + res[r] )\n",
    "            if( i == res[r] ):\n",
    "                count_set[0,cnt] += 1\n",
    "        cnt += 1\n",
    "#     print(count_set) ##Frequency of Classes\n",
    "    \n",
    "    class_max = np.max(count_set)\n",
    "#     print(class_max)\n",
    "\n",
    "    idx_max = 0\n",
    "    for i in range(count_set.shape[1]):\n",
    "        if( class_max == count_set[0,i] ):\n",
    "            idx_max = i\n",
    "#     print(idx_max)\n",
    "\n",
    "    set_class = list(set_class)\n",
    "    # print(set_class[0])\n",
    "#     print(set_class[idx_max])\n",
    "    return set_class[idx_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_idx_item( X, idx ):\n",
    "    X_tmp = []\n",
    "    for ix in range(0,X.shape[0]):\n",
    "        if( ix != idx ):\n",
    "            X_tmp.append( X[ix,:] )\n",
    "    X_tmp = np.array(X_tmp)\n",
    "    \n",
    "    return X_tmp\n",
    "\n",
    "import random\n",
    "def insert_random_point( X ):\n",
    "    X_tmp = np.zeros([ X.shape[0]+1, X.shape[1]])\n",
    "    \n",
    "    X_tmp[0:X.shape[0],:] = X[:,:]\n",
    "    \n",
    "    for i in range( 0, X.shape[1]):\n",
    "        X_tmp[ int(X.shape[0]), i] = random.random()\n",
    "    \n",
    "    return X_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Inserção sequencial\n",
    "def insercao(iris_data, function):\n",
    "    data = iris_data.get_values()\n",
    "#     nc=len(data.columns)\n",
    "#     print( data[0,0:data.shape[1]-1] )\n",
    "#     print(type(data))\n",
    "    nc = iris_data.shape[1]\n",
    "    store = []\n",
    "    grab_bag = []\n",
    "\n",
    "    ##Step 1\n",
    "    store.append( data[0,:] )\n",
    "    # print(store[:][0:5])\n",
    "\n",
    "    ##Step 2, 3 \n",
    "    for i in range( 1, data.shape[0]):\n",
    "        ar_store = np.array(store)\n",
    "        x = data[i,0:data.shape[1]-1]\n",
    "\n",
    "        ##Removing i item\n",
    "        ar_store = remove_idx_item( ar_store, i )\n",
    "        if( data[ i, data.shape[1]-1] == my_KNN( ar_store[:,0:nc-1], ar_store[:,nc-1], x, 1, function)):\n",
    "            grab_bag.append(data[i,:])\n",
    "        else:\n",
    "            store.append(data[i,:])\n",
    "\n",
    "    ##Step 4\n",
    "    size_gbg = np.array(grab_bag).shape        \n",
    "    for i in range( 1, size_gbg[0]):\n",
    "        ar_store = np.array(store)\n",
    "        ar_grab_bag = np.array(grab_bag)\n",
    "        x = ar_grab_bag[i,0:ar_grab_bag.shape[1]-1]\n",
    "        if( ar_grab_bag[ i, ar_grab_bag.shape[1]-1] == my_KNN( ar_store[:,0:nc-1], ar_store[:,nc-1], x, 1, function)):\n",
    "            grab_bag.append(data[i,:])\n",
    "        else:\n",
    "            store.append(data[i,:])\n",
    "    ar_store = np.array(store)\n",
    "#     print(ar_store.shape)\n",
    "    # print(ar_store[ar_store[:,0].argsort()])\n",
    "\n",
    "    output = []\n",
    "    nc = data.shape[1]\n",
    "    for i in range(0,data.shape[0]):\n",
    "        x = data[i,0:nc-1]\n",
    "    #     print(x)\n",
    "        y_est = my_KNN( ar_store[:,0:nc-1], ar_store[:,nc-1], x, 1, function)\n",
    "        output.append( [ y_est, data[i,nc-1] ]  )\n",
    "#     print(['y_est, y'])\n",
    "    output = np.array(output)\n",
    "#     print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiClassConfusionMatrix(output[:,1],output[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Eliminação sequencial\n",
    "def eliminicao(iris_data, function):\n",
    "    data = iris_data.get_values()\n",
    "    nc = data.shape[1]\n",
    "#     print( data[0,0:data.shape[1]-1] )\n",
    "#     print(type(data))\n",
    "    store = []\n",
    "    grab_bag = []\n",
    "\n",
    "    ##Step 1\n",
    "    for i in range(0, data.shape[0]):\n",
    "        store.append( data[i,:] )\n",
    "\n",
    "    ##Step 2, 3\n",
    "    a_store = np.array(store)\n",
    "    # print('ast_ ' + ' ' + str(a_store.shape[0]) )\n",
    "    for i in range( 0, data.shape[0]):\n",
    "\n",
    "        x = data[i,0:data.shape[1]-1]\n",
    "\n",
    "        ##Removing i item\n",
    "        ar_store = remove_idx_item( a_store, i )\n",
    "    #     print( str(i) + ' ' + str(ar_store.shape[0]) )\n",
    "\n",
    "    #     print( str(data[ i, data.shape[1]-1]) + ' ' + str(my_KNN( ar_store[:,0:4], ar_store[:,4], x, K = 1, function = my_mse)) )\n",
    "\n",
    "        if( data[ i, data.shape[1]-1] == my_KNN( ar_store[:,0:nc-1], ar_store[:,nc-1], x, K = 1, function = function)):\n",
    "            a_store = remove_idx_item( a_store, i)\n",
    "\n",
    "#     print(a_store.shape)\n",
    "    \n",
    "    output = []\n",
    "    nc = data.shape[1]\n",
    "    for i in range(0,data.shape[0]):\n",
    "        x = data[i,0:nc-1]\n",
    "    #     print(x)\n",
    "        y_est = my_KNN( ar_store[:,0:nc-1], ar_store[:,nc-1], x, K = 1, function = function )\n",
    "        output.append( [ y_est, data[i,nc-1] ]  )\n",
    "#     print(['y_est, y'])\n",
    "    output = np.array(output)\n",
    "#     print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiClassConfusionMatrix(output[:,1],output[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_kbest.head()\n",
    "label =pd.DataFrame(data = cn_features.get_values()[:,9])\n",
    "label.columns = ['10']\n",
    "data_kbest = pd.concat([data_kbest, label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197665</td>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214964</td>\n",
       "      <td>0.221916</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162983</td>\n",
       "      <td>0.185103</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242440</td>\n",
       "      <td>0.238390</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263409</td>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         3         5         7   10\n",
       "0  0.197665  0.208098  0.835600  0.647579  0.572138  0.0\n",
       "1  0.214964  0.221916  0.861376  0.620801  0.459421  0.0\n",
       "2  0.162983  0.185103  0.814755  0.713141  0.364839  0.0\n",
       "3  0.242440  0.238390  0.809617  0.570203  0.603046  0.0\n",
       "4  0.263409  0.262010  0.821811  0.552144  0.668801  0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_kbest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =pd.DataFrame(data = cn_features.get_values()[:,9])\n",
    "data_corr = pd.concat([data_corr, label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.221916</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.185103</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.238390</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         3         5         6         8    0\n",
       "0  0.208098  0.835600  0.647579  0.226739  0.572138  0.0\n",
       "1  0.221916  0.861376  0.620801  0.237816  0.459421  0.0\n",
       "2  0.185103  0.814755  0.713141  0.248812  0.364839  0.0\n",
       "3  0.238390  0.809617  0.570203  0.229935  0.603046  0.0\n",
       "4  0.262010  0.821811  0.552144  0.224306  0.668801  0.0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_corr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =pd.DataFrame(data = cn_features.get_values()[:,9])\n",
    "data_lvf = pd.concat([data_lvf, label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197665</td>\n",
       "      <td>0.835600</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.226739</td>\n",
       "      <td>0.572138</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.214964</td>\n",
       "      <td>0.861376</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.237816</td>\n",
       "      <td>0.459421</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.162983</td>\n",
       "      <td>0.814755</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.248812</td>\n",
       "      <td>0.364839</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242440</td>\n",
       "      <td>0.809617</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.229935</td>\n",
       "      <td>0.603046</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.263409</td>\n",
       "      <td>0.821811</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.224306</td>\n",
       "      <td>0.668801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         3         5         6         7    0\n",
       "0  0.197665  0.835600  0.647579  0.226739  0.572138  0.0\n",
       "1  0.214964  0.861376  0.620801  0.237816  0.459421  0.0\n",
       "2  0.162983  0.814755  0.713141  0.248812  0.364839  0.0\n",
       "3  0.242440  0.809617  0.570203  0.229935  0.603046  0.0\n",
       "4  0.263409  0.821811  0.552144  0.224306  0.668801  0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lvf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =pd.DataFrame(data = cn_features.get_values()[:,9])\n",
    "data_wrapper = pd.concat([data_wrapper, label], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.208098</td>\n",
       "      <td>0.793149</td>\n",
       "      <td>0.647579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.221916</td>\n",
       "      <td>0.799994</td>\n",
       "      <td>0.620801</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.185103</td>\n",
       "      <td>0.842367</td>\n",
       "      <td>0.713141</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.238390</td>\n",
       "      <td>0.886523</td>\n",
       "      <td>0.570203</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.262010</td>\n",
       "      <td>0.835281</td>\n",
       "      <td>0.552144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         5    0\n",
       "0  0.208098  0.793149  0.647579  0.0\n",
       "1  0.221916  0.799994  0.620801  0.0\n",
       "2  0.185103  0.842367  0.713141  0.0\n",
       "3  0.238390  0.886523  0.570203  0.0\n",
       "4  0.262010  0.835281  0.552144  0.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wrapper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(4, 6)\n",
      "eliminicao secuencial\n",
      "[0.19766497 0.20809772 0.8356     0.64757921 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(4, 6)\n",
      "eliminicao secuencial\n",
      "[0.19766497 0.20809772 0.8356     0.64757921 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(423, 6)\n",
      "eliminicao secuencial\n",
      "[0.19766497 0.20809772 0.8356     0.64757921 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(3, 6)\n",
      "eliminicao secuencial\n",
      "[0.20809772 0.8356     0.64757921 0.22673947 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(3, 6)\n",
      "eliminicao secuencial\n",
      "[0.20809772 0.8356     0.64757921 0.22673947 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(423, 6)\n",
      "eliminicao secuencial\n",
      "[0.20809772 0.8356     0.64757921 0.22673947 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(3, 6)\n",
      "eliminicao secuencial\n",
      "[0.19766497 0.8356     0.64757921 0.22673947 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(3, 6)\n",
      "eliminicao secuencial\n",
      "[0.19766497 0.8356     0.64757921 0.22673947 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(423, 6)\n",
      "eliminicao secuencial\n",
      "[0.19766497 0.8356     0.64757921 0.22673947 0.57213823]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(4, 4)\n",
      "eliminicao secuencial\n",
      "[0.20809772 0.79314895 0.64757921]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n",
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(4, 4)\n",
      "eliminicao secuencial\n",
      "[0.20809772 0.79314895 0.64757921]\n",
      "<class 'numpy.ndarray'>\n",
      "Subsampling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jecs89/anaconda3/envs/tutorialConda/lib/python3.6/site-packages/ipykernel_launcher.py:28: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold cross validation\n",
      "Insercion secuencial\n",
      "(423, 4)\n",
      "eliminicao secuencial\n",
      "[0.20809772 0.79314895 0.64757921]\n",
      "<class 'numpy.ndarray'>\n",
      "[0.99716981 1.         0.99716981] [0.99716981 1.         0.99716981] [0.99716981 1.         0.99716981] \n",
      "[0.99767442 1.         0.99767442] [0.99767442 1.         0.99767442] [0.99767442 1.         0.99767442] \n",
      "[1. 1. 1.] [1. 1. 1.] [1. 1. 1.] \n",
      "[1. 1. 1.] [1. 1. 1.] [1. 1. 1.] \n"
     ]
    }
   ],
   "source": [
    "distances = [ my_dis1, my_dis2, my_dis3 ]\n",
    "#functions = [randomSubSampling, kFoldCrossValidation, leaveOneOut, insercao, eliminicao ]\n",
    "\n",
    "datasets = [ data_kbest, data_corr, data_lvf, data_wrapper ]\n",
    "\n",
    "table = []\n",
    "\n",
    "for data in datasets:\n",
    "    for d in distances:\n",
    "        print('Subsampling')\n",
    "        test_size = np.floor(0.25*424)\n",
    "        test_size=int(test_size)\n",
    "        prom = randomSubSampling(test_size,d,data)\n",
    "        table.append(prom)\n",
    "    \n",
    "        print('K-fold cross validation')\n",
    "        k_fold = 10\n",
    "        prom = kFoldCrossValidation(k_fold,d,data)\n",
    "        table.append(prom)\n",
    "\n",
    "#         print('Leave one out')\n",
    "#         leaveOneOut(d,data)\n",
    "\n",
    "        print('Insercion secuencial')\n",
    "        output = insercao(data,d)\n",
    "        prom = MultiClassConfusionMatrix(output[:,1],output[:,0])\n",
    "        table.append(prom)\n",
    "\n",
    "        print('eliminicao secuencial')\n",
    "        output = eliminicao(data,d)\n",
    "        prom = MultiClassConfusionMatrix(output[:,1],output[:,0])\n",
    "        table.append(prom)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99716981 1.         0.99716981] [0.99767442 1.         0.99767442] [1. 1. 1.] [1. 1. 1.] [0.9990566 1.        0.9990566] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [0.56792453 0.         0.43207547] [0.54380893 0.         0.45619107] [0.0259434  0.0259434  0.94811321] [0.0259434  0.0259434  0.94811321] \n",
      "\n",
      "[1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [0.5509434  0.00471698 0.44433962] [0.54380893 0.00232558 0.45386549] [0.22877358 0.00471698 0.76650943] [0.22877358 0.00471698 0.76650943] \n",
      "\n",
      "[1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [1. 1. 1.] [0.52641509 0.00283019 0.47075472] [0.54380893 0.00232558 0.45386549] [0.22169811 0.         0.77830189] [0.22169811 0.         0.77830189] \n",
      "\n",
      "[0.9990566 1.        0.9990566] [0.99767442 1.         0.99767442] [1. 1. 1.] [1. 1. 1.] [0.99811321 1.         0.99811321] [0.99767442 1.         0.99767442] [1. 1. 1.] [1. 1. 1.] [0.08018868 0.         0.91981132] [0.05191703 0.         0.94808297] [0.53773585 0.23113208 0.23113208] [0.53773585 0.23113208 0.23113208] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for data in range(len(datasets)):\n",
    "    for d in range(len(distances)):\n",
    "        print( table[i], end = ' '  )\n",
    "        i += 1\n",
    "        print( table[i], end = ' '  )\n",
    "        i += 1\n",
    "        print( table[i], end = ' '  )\n",
    "        i += 1\n",
    "        print( table[i], end = ' '  )\n",
    "        i += 1\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99716981 1.         0.99716981]\n",
      " [0.99767442 1.         0.99767442]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.9990566  1.         0.9990566 ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.56792453 0.         0.43207547]\n",
      " [0.54380893 0.         0.45619107]\n",
      " [0.0259434  0.0259434  0.94811321]\n",
      " [0.0259434  0.0259434  0.94811321]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.5509434  0.00471698 0.44433962]\n",
      " [0.54380893 0.00232558 0.45386549]\n",
      " [0.22877358 0.00471698 0.76650943]\n",
      " [0.22877358 0.00471698 0.76650943]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.52641509 0.00283019 0.47075472]\n",
      " [0.54380893 0.00232558 0.45386549]\n",
      " [0.22169811 0.         0.77830189]\n",
      " [0.22169811 0.         0.77830189]\n",
      " [0.9990566  1.         0.9990566 ]\n",
      " [0.99767442 1.         0.99767442]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.99811321 1.         0.99811321]\n",
      " [0.99767442 1.         0.99767442]\n",
      " [1.         1.         1.        ]\n",
      " [1.         1.         1.        ]\n",
      " [0.08018868 0.         0.91981132]\n",
      " [0.05191703 0.         0.94808297]\n",
      " [0.53773585 0.23113208 0.23113208]\n",
      " [0.53773585 0.23113208 0.23113208]]\n"
     ]
    }
   ],
   "source": [
    "print(np.array(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "print(len(table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
